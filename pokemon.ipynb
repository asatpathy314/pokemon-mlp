{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import os   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Type1', 'Type2', 'Evolution', 'Image_Data'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_images(folder_name):\n",
    "    names = []\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_name):\n",
    "        try:\n",
    "            filepath = os.path.join(folder_name, filename)\n",
    "            img = Image.open(filepath)\n",
    "            img = np.asarray(img).flatten()\n",
    "            names.append(filename.split(\".\")[0])\n",
    "            images.append(img)\n",
    "        except Exception as error:\n",
    "            print(f\"Error loading {folder_name}: {error}\")\n",
    "    return dict(zip(names, images)) # Returns a dictionary of names to image data\n",
    "\n",
    "image_data = \"images\"\n",
    "pokemon_data = pd.read_csv(\"pokemon.csv\")\n",
    "names_to_images = load_images(image_data)\n",
    "\n",
    "# add image data column using dictionary and the \"Name\" column in the dataframe\n",
    "pokemon_data['Image_Data'] = pokemon_data['Name'].map(names_to_images)\n",
    "pokemon_data.columns\n",
    "pokemon_data.loc[lambda x: x.Type2.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Bayesian Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem \n",
    "Bayes theorem defines the probability of an event $A$ conditioned on an event $B$ as follows.\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B | A) \\ast P(A)}{P(B)} $$\n",
    "\n",
    "Using this theorem, we can optimize a complicated function by creating an easier to calculate surrogate based on a probability distribution created by sampling our original function. Then we continue to take samples that are most likely to improve our estimate of the best \"score\", making our surrogates estimations more and more accurate. This process is called **Bayesian Optimization**. Let's get started by defining an \"objective\" function (score function) and a basic Gaussian Process model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an expensive objective function that we want to approximate\n",
    "def objective(x, noise=0.5):\n",
    "    random_noise = np.random.normal(scale=noise)\n",
    "    return (math.sin(3* math.pi * x) + x) + random_noise\n",
    "\n",
    "# Generate initial samples with noise\n",
    "X = np.random.random(100).reshape(-1, 1)\n",
    "y = np.array([objective(x) for x in X]).reshape(-1, 1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = GaussianProcessRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a visualization function that plots prediction vs. actual\n",
    "def visualize(X, y):\n",
    "    plt.scatter(X, y)\n",
    "    Xsamples = np.arange(0, 1, 0.001).reshape(-1, 1)\n",
    "    ysamples = model.predict(Xsamples)\n",
    "    plt.title(\"Initial Model Predictions\")\n",
    "    plt.plot(Xsamples, ysamples)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the initial data\n",
    "visualize(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition Function\n",
    "Now we have the problem of exploration vs exploitation, often observed in reinforcement learning problems such as bandit optimization. How can we choose points such that we optimize the balance between exploring unknown points that could potentially be better than what we have and verifying our current highest scores, especially in a non-deterministic sample space? This is where acquisition functions come in, they use various heuristic methods to determine what points are most likely to improve the model's predictions. Some of these methods include:\n",
    "\n",
    "- Probability of Improvement (PI): This is the simplest acquisition function. It focuses on the probability that a given point will improve upon the current best-known value. However, it often gets stuck in local optima due to its simplicity.\n",
    "- Expected Improvement (EI): This function not only considers the probability of improvement but also takes into account the potential magnitude of improvement. It refines the approach by evaluating how much better a possible outcome could be.\n",
    "- Entropy Search: This method selects points that will provide the most information gain, effectively balancing exploration across the sample space. It can be likened to traversing an extensive landscape of hills and valleys, seeking areas that offer the most insight.\n",
    "- Upper Confidence Bound (UCB): UCB uses a balance factor to weigh between exploration and exploitation. It adjusts its strategy based on a confidence interval around predictions, allowing for a dynamic balance between trying new points and exploiting known high-value areas.\n",
    "\n",
    "The technical workings of why these functions work and exactly how they differ from each other are beyond the scope of this class. However, it's worth knowing the functional differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the acquisition function to calculate probability of improvement\n",
    "def probability_of_improvement(X, Xsamples, xi=0.01):\n",
    "    # Determine the best score from the surrogate model so far\n",
    "    yhat = model.predict(X)\n",
    "    best_score = max(yhat)\n",
    "\n",
    "    # Calculate mean and standard deviation using the surrogate model\n",
    "    mu, std = model.predict(Xsamples, return_std=True)\n",
    "\n",
    "    # Compute probability of improvement\n",
    "    probabilities = norm.cdf((mu - best_score - xi) / (std + 1E-9))\n",
    "    return probabilities\n",
    "\n",
    "# Function to optimize the acquisition function\n",
    "def optimize_acquisition(X, y):\n",
    "    # Generate random samples for exploration\n",
    "    Xsamples = np.random.random(100).reshape(-1, 1)\n",
    "    # Evaluate acquisition scores for each sample\n",
    "    scores = probability_of_improvement(X, Xsamples)\n",
    "    # Identify the sample with the highest score\n",
    "    best_index = np.argmax(scores)\n",
    "    return Xsamples[best_index, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization loop to improve sampling points iteratively\n",
    "for i in range(100):\n",
    "    next_sample = optimize_acquisition(X, y)\n",
    "    actual_value = objective(next_sample)\n",
    "    \n",
    "    # Update dataset with new sample point and observation\n",
    "    X = np.vstack((X, [[next_sample]]))\n",
    "    y = np.vstack((y, [[actual_value]]))\n",
    "    \n",
    "    # Re-fit model with updated data\n",
    "    model.fit(X, y)\n",
    "\n",
    "# Final plot of all samples and surrogate model predictions\n",
    "visualize(X, y)\n",
    "\n",
    "# Identify and print the best result found during optimization\n",
    "best_index = np.argmax(y)\n",
    "print('Best Result: x=%.3f, y=%.3f' % (X[best_index], y[best_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
